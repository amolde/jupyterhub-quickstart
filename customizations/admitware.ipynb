{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "#Create spark configuration object\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"My app\")\n",
    " \n",
    "#Create spark context and sparksession\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#set variable to be used to connect the database\n",
    "user = \"\"\n",
    "password  = \"\"\n",
    "host = \"\"\n",
    "port = \"\"\n",
    "database = \"admitware\"\n",
    "# database = \"Inquiry\"\n",
    "# # database = 'FWNEUCASE'\n",
    "\n",
    "# user = \"\"\n",
    "# password = \"\"\n",
    "# host = \"\"\n",
    "# port = \"\"\n",
    "# database = \"Admissions\"\n",
    "\n",
    "# %set_env PYSPARK_SUBMIT_ARGS=--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"INFORMATION_SCHEMA.TABLES\"\n",
    "# table = \"Application\"\n",
    "# table = \"Student\"\n",
    "# table = \"DBFieldName\"\n",
    "# table = \"Inquiry\"\n",
    " \n",
    "#read table data into a spark dataframe\n",
    "jdbcDF = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{host}:{port};databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .load()\n",
    " \n",
    "#show the data loaded into dataframe\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl=f\"jdbc:sqlserver://{host}:{port};database={database}\"\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\" : user,\n",
    "  \"password\" : password,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# jdbc_query = \"select * from INFORMATION_SCHEMA.TABLES where TABLE_TYPE = 'BASE TABLE'\"\n",
    "jdbc_query = \"select * from INFORMATION_SCHEMA.columns WHERE column_NAME LIKE '%admissionsdataid%'\"\n",
    "# jdbc_query = \"select * from commonapp where First_name = 'Kaiyan'\"\n",
    "jdbc_table = \"(\" + jdbc_query + \") base_tables_alias\"\n",
    "partition_column = 'TABLE_NAME'\n",
    "\n",
    "df = spark.read.jdbc( \\\n",
    "        url=jdbcUrl, \\\n",
    "        table=jdbc_table, \\\n",
    "                     \n",
    "#         column=partition_column, \\ # maximal number of concurrent JDBC connections\n",
    "#         lowerBound=1, \\\n",
    "#         upperBound=100000, \\\n",
    "#         numPartitions=100, \\\n",
    "                     \n",
    "#         predicates = [\"TABLE_TYPE = 'BASE TABLE'\" ], \\\n",
    "        numPartitions = 1, \\\n",
    "        properties=connectionProperties \\\n",
    "    )\n",
    "# df.show()\n",
    "# df.show(200)\n",
    "# display(df)\n",
    "# display(df.limit(20))\n",
    "pd_df = df.limit(4000).toPandas().sort_values(by=['TABLE_NAME'])\n",
    "# display(pd_df.head(1000).tail(300))\n",
    "display(pd_df.head(3000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jdbcUrl=f\"jdbc:sqlserver://{host}:{port};database={database}\"\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\" : user,\n",
    "  \"password\" : password,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# query_table = 'Application'\n",
    "\n",
    "# possible candidates...\n",
    "# query_table = 'Inquiry'\n",
    "# query_table = 'INQCase'\n",
    "# query_table = 'SPECCase'\n",
    "# query_table = 'NEUCase'\n",
    "\n",
    "# query_table = 'StudentDocHistory'\n",
    "# query_table = \"GeoMarket\"\n",
    "\n",
    "###################\n",
    "\n",
    "query_table = \"NEUCASE\"\n",
    "\n",
    "jdbc_query = f\"select top(100) * from {query_table} where NUID = '001387092'\"\n",
    "# jdbc_query = f\"select top(100) * from {query_table} where admissionsdataid = 'N0071283103193' order by TermYear desc\"\n",
    "\n",
    "jdbc_table = \"(\" + jdbc_query + \") base_tables_alias\"\n",
    "# partition_column = 'TABLE_NAME'\n",
    "\n",
    "df = spark.read.jdbc( \\\n",
    "        url=jdbcUrl, \\\n",
    "        table=jdbc_table, \\\n",
    "                     \n",
    "#         column=partition_column, \\ # maximal number of concurrent JDBC connections\n",
    "#         lowerBound=1, \\\n",
    "#         upperBound=100000, \\\n",
    "#         numPartitions=100, \\\n",
    "                     \n",
    "#         predicates = [\"TABLE_TYPE != 'BASE TABLE'\", \"TABLE_TYPE = 'BASE TABLE'\" ], \\\n",
    "#         numPartitions = 1, \\\n",
    "        properties=connectionProperties \\\n",
    "    )\n",
    "# df.show(200)\n",
    "# display(df)\n",
    "# display(df.limit(20))\n",
    "\n",
    "pd_df = df.limit(2000).toPandas()\n",
    "# display(pd_df[['ACT_Composite_Score', 'IB_Subject_total_number', 'IELTS_Overall_Band_Score', 'SAT_CR_Score', 'SAT_Math_Score', 'SAT_Subject_total_number']])\n",
    "# print(pd_df)\n",
    "display(pd_df)\n",
    "\n",
    "# print(pd_df)\n",
    "\n",
    "for c in pd_df.columns:\n",
    "    print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "f\"jdbc:sqlserver://{os.environ.get('ADMITWARE_HOST')}:{os.environ.get('ADMITWARE_PORT')};database={os.environ.get('ADMITWARE_DATABASE')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, sql\n",
    "\n",
    "# sc = SparkContext('local', 'Test')\n",
    "# spark = sql.SparkSession \\\n",
    "# .builder \\\n",
    "# .appName(\"TEST\") \\\n",
    "# .getOrCreate()\n",
    "\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.multipart.uploads.enabled\", \"true\")\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AssumedRoleCredentialProvider\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.assumed.role.arn\", \"arn:aws:iam::209299999084:role/ep-test-compute-node-role\")\n",
    "\n",
    "sql_context = sql.SQLContext(sc, spark)\n",
    "filename = 'admitware/test.parquet'\n",
    "s3_uri = 's3a://nu-data-lake-test/{}'.format(filename)\n",
    "print(s3_uri)\n",
    "df = sql_context.createDataFrame([('1', '4'), ('2', '5'), ('3', '6')], [\"A\", \"B\"])\n",
    "df.write.parquet(s3_uri)\n",
    "# df.write.parquet(\"s3a://nu-data-lake-test/admitware/test.parquet\",mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}