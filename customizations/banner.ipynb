{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required modules\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "#Create spark configuration object\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"My app\")\n",
    " \n",
    "#Create spark context and sparksession\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "host = \"\"\n",
    "port = \"\"\n",
    "database = \"\"\n",
    "\n",
    "# %set_env PYSPARK_SUBMIT_ARGS=--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"ALL_TABLES\"\n",
    "# table = \"Application\"\n",
    "# table = \"Student\"\n",
    "# table = \"DBFieldName\"\n",
    "# table = \"Inquiry\"\n",
    " \n",
    "#read table data into a spark dataframe\n",
    "jdbcDF = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:oracle:thin:@{host}:{port}:{database}\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n",
    "    .load()\n",
    " \n",
    "#show the data loaded into dataframe\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl=f\"jdbc:oracle:thin:@{host}:{port}:{database}\"\n",
    "\n",
    "connectionProperties = {\n",
    "  \"user\" : user,\n",
    "  \"password\" : password,\n",
    "  \"driver\" : \"oracle.jdbc.driver.OracleDriver\"\n",
    "}\n",
    "\n",
    "query_table = 'STVCOLL'\n",
    "\n",
    "jdbc_query = f\"select * from {query_table}\"\n",
    "jdbc_table = \"(\" + jdbc_query + \") base_tables_alias\"\n",
    "\n",
    "# partition_column = 'TABLE_NAME'\n",
    "\n",
    "df = spark.read.jdbc( \\\n",
    "        url=jdbcUrl, \\\n",
    "        table=jdbc_table, \\\n",
    "#         column=partition_column, \\ # maximal number of concurrent JDBC connections\n",
    "#         lowerBound=1, \\\n",
    "#         upperBound=100000, \\\n",
    "#         numPartitions=100, \\\n",
    "#         predicates = [\"TABLE_TYPE != 'BASE TABLE'\", \"TABLE_TYPE = 'BASE TABLE'\" ], \\\n",
    "#         numPartitions = 1, \\\n",
    "        properties=connectionProperties \\\n",
    "    )\n",
    "df.show(200)\n",
    "# display(df)\n",
    "# display(df.limit(20))\n",
    "# display(df.limit(2000).toPandas())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}